<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Project - Elo Eschatology</title>
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta property="og:title" content="Elo-schatology">
        <meta property="og:author" content="Michael Anderson">
        <link rel="stylesheet" href="res/css/bootstrap.min.css">
        <link rel="stylesheet" href="res/css/main.css">
        <script src="res/js/jquery.min.js"></script>
        <script src="res/js/bootstrap.min.js"></script>
    </head>
    <body>

        <ul id="m_navbar">
            <li><div class="navbar_item"><a href="index.html">Home</a></div></li>
                <li class="dropdown" id="active"><div class="navbar_item">Projects</div>
                    <div class="dropdown-content">
                        <div class="navbar_item"><a href="argusclient.html">Argus Client</a></div>
                        <div class="navbar_item"><a href="CREAMserver.html">CREAM Server</a></div>
                        <div class="navbar_item"><a href="ID3predictor.html">ID3 Predictor</a></div>
                        <div class="navbar_item" id="active"><a href="Eloschatology.html">Elo Simulator</a></div>
                    </div>
                </li>
            <li><div class="navbar_item"><a href="MikeAndersResume2018.pdf">Resume</a></li></div>
        </ul>

        <div class="row">

            <div class="col-sm-8">
                <div id="text-block">
                    <p>
                        Recently I've put together a python project that tests <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo matchmaking</a> in a simulated environment. Elo matchmakers, for those unfamiliar, are used in competitive games like chess. They can choose who plays who, and by observing who wins or loses, must guess each player's true skill. These systems suffer a common complaint: players will grind hundreds of games only to see little improvement in their score or ranking. Is the matchmaker to blame? Can Elo systems really misplace people badly, and are they vulnerable to 'Elo Hell' - persistant bands of players with closely estimated skills, but widely disparate true skills, making for random and frustrating matches - or are these complaints another example of <a href="https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect">Dunningâ€“Kruger</a> syndrome?
                   </p>
                   <p>
                        To test this, I went to <a href="https://ratings.fide.com/">FIDE</a>, who helpfully have a public database of all professional chess player's Elo ratings. Although FIDE puts a hard floor on all rankings, using numpy we can create a randomized distribution that roughly mimics the actual score distribution. These will be our players' true scores.
                   </p>
                   <p>
                       Next, we can initialize our estimated rankings by setting all players to the same rank (here 1690, the median chess ranking). All that's left is to have players match against each other in an initial free-for-all.
                   </p>
                </div>
                   
                <div id="inline-block">
                    <div class="snippet">
                        <pre><code>
<span class="kwd2">import</span> EloSim <span class="kwd2">as</span> e
<span class="cmt"># generate players with a randomized true-skill distribution</span>
players = e.initplayers()
<span class="cmt"># Have players match against each other to estimate rank. Because all players are initialized with the same estimated Elo, we should match freely without skill-brackets</span>
playtournament(players, <span class="arg">team_size</span>=1, <span class="arg">uncertainty</span>=32, <span class="arg">learning_rate</span>=1.0, <span class="arg">rounds</span>=20, <span class="arg">usebrackets</span>=False)
                        </code></pre>
                    </div>
                </div>
                   
                   
                <div id="text-block">
                   <p>
                        And the player base is initialized! We can see the estimated skill distribution take shape within 20 rounds of play.
                   </p>
                </div>
            
                <div id="inline-block">
                    <img class="img-thumbnail" width="45%" src="res/img/Elo/gen/Rd10_dist.png">
                    <img class="img-thumbnail" width="45%" src="res/img/Elo/gen/Rd20_dist.png">
                </div>

                <div id="text-block">
                    <p>
                        With a rough estimate of player skills, we can now start making educated matches, where players of similarly estimated skill can face each other. We will bucket players into 23 skill brackets (as is common in most chess tournaments), allowing for some play outside the brackets. To measure our prediction accuracy, we will look at the overall skill spread, and at the number of matches it would take to move players from their estimated ranked ordering to their true ranked ordering.
                    </p>
                </div>
                    
                <div id="inline-block">
                    <div class="snippet">
                        <pre><code>
<span class="cmt"># Play a 100 round tournament where players are roughly matched according to their skill level</span>
play_tournament(players, <span class="arg">team_size</span>=1, <span class="arg">uncertainty</span>=32, <span class="arg">learning_rate</span>=0.995, <span class="arg">rounds</span>=100, <span class="arg">usebrackets</span>=True)
                        </code></pre>
                    </div>
                </div>
                    
                <div id="text-block">
                    <p>
                        We quickly see our distribution tighten to match a Guassian approximation of the actual of FIDE ratings.
                    </p>
                </div>

                <div id="inline-block">
                    <img class="img-thumbnail" width="45%" src="res/img/Elo/chess_rankings.png">
                    <img class="img-thumbnail" width="45%" src="res/img/Elo/test1/Rd25_dist.png">
                </div>

                <div id="text-block">
                    <p>
                        To measure accuracy, we will look at the distance between a player's estimated ranked ordering and their true ranked ordering. This distance can be measured in the expected number of matches to move to the true ranked order.
                    </p>
                </div>

                <div id="inline-block">
                    <div class="snippet">
                        <pre><code>
<span class="cmt"># get the elo distance from true rank</span>
raw_dist = PUB_ELOS[<span class="kwd1">self</span>.id] - PUB_ELOS[playerlist[<span class="kwd1">self</span>.id].id]
expected_match_val = <span class="typ">float</span>((true_winchance*win_amount)) + <span class="typ">float</span>((1.0 - true_winchance)*loss_amount)
<span class="kwd1">self</span>.rank_dist = raw_dist / expected_match_val
                        </code></pre>
                    </div>
                </div>

                <div id="text-block">
                    <p>
                            Below we can see that the tournament from before yielded roughly accurate player rankings, with a moderate (but persistent) margin of error across all skill levels. Even in seperate tests where I expanded the team size to 5 person teams (used in online games like League of Legends), the results are not significantly different. Error margins were moderate and persistent across all skill bands, with the greatest fluctuations at the tails of the distribution where players are sparse.
                    </p>
                </div>

                <div id="inline-block">
                    <img class="img-thumbnail" width="45%" src="res/img/Elo/test1/Rd25_sprd.png">
                    <img class="img-thumbnail" width="45%" src="res/img/Elo/test1/Rd25_err.png">

                    <img class="img-thumbnail" width="45%" src="res/img/Elo/test1/Rd75_sprd.png">
                    <img class="img-thumbnail" width="45%" src="res/img/Elo/test1/Rd75_err.png">
                </div>

                <div id="text-block">
                    <p>
                        There is an insidious detail of getting to one's true rank. Although players average about 30 matches from their true ranked order, the number of consecutive wins or losses it would take is much lower, around 4. While four consecutive wins is unlikely against similarly skilled opposition, this isn't always intuitive. Slot machines take advantage of this by frequently showing an off-by-one Jackpot miss, convincing players that a big payout is likely. Elo systems that have strong bracketing with prestigious ranks, or that display points won or lost after a match, are probably similarly addictive. This might explain the frustrated players who see their easy expectations meet hard reality.
                    </p>
                </div>

                <div id="inline-block">
                    <figure style="width:45%;">
                        <img class="img-thumbnail" src="res/img/slot.jpg">
                        <figcaption> A 'near miss' - Image courtesy of <a href="https://www.vox.com/">Vox.com</a> </figcaption>
                    </figure>
                    <figure style="width:45%;">
                        <img class="img-thumbnail" src="res/img/Elo/test1/Rd75_errW.png">
                        <figcaption> Deceptively far. 3 matches is more likely to be 30 </figcaption>
                    </figure>
                </div>

                <div id="text-block">
                    <p>
                        So far we've used wide matchmaking bands, matching players with anyone within 435 (10000 players / 23 bands) places in the sorted ranking. Let's switch to a stricter match-maker that attempts to pair players within tighter bands.
                    </p>
                </div>

                <div id="inline-block">
                    <img class="img-thumbnail" width="30%" src="res/img/Elo/test2/Rd50_err.png">
                    <img class="img-thumbnail" width="30%" src="res/img/Elo/test2/Rd75_err.png">
                    <img class="img-thumbnail" width="30%" src="res/img/Elo/test2/Rd100_err.png">
                </div>

                <div id="text-block">
                    <p>
                        While tighter matching will initially have a wider skill spread and more randomness, it eventually converges to a much narrower error variance than the more randomized matchmaking. Tuning the learning rate and uncertainty has a similar effect - making extreme (good and bad) luck streaks less impactful, and making the system converge toward lower error margins and lower variance.
                    </p>
                </div>

                <div id="inline-block">
                    <p></p><br/>
                    <p></p><br/>
                    <a href="https://github.com/milanderson/EloSim">GitHub</a><br>
                    <a href="https://ratings.fide.com/download.phtml">DataSet</a>
                </div>
            </div>

            <div class="col-sm-4">
            </div>

        </div>

        <footer>
            <div class="container">
                <div class="col-sm-10" id="m_footer">
                    <hr>
                    Â© Michael Anderson 2018 - Powered by <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>
                </div>
            </div>
        </footer>

    </body>
</html>
